#coding=utf-8
'''
Created on 2017��3��30��

@author: gb
'''
import tensorflow as tf
import opendata
import gensim
import numpy as np 

def weight_variable(shape):
    initial=tf.truncated_normal(shape,stddev=0.1)
    return tf.Variable(initial)
def bias_variable(shape):
    initial=tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
def conv2d(x,w):
    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME' )
def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

x = tf.placeholder(tf.float32, [None, 200])#question 占位
x_reshape=tf.reshape(x,[-1,10,20,1])  #问题输入变形

r = tf.placeholder(tf.float32, [None, 200])#answer 占位
r_reshape=tf.reshape(r,[-1,10,20,1])        #答案输入变形

y=tf.placeholder(tf.float32,[None,1])#标签占位
##conv1##
w_conv1=weight_variable([5,5,1,32])
b_conv1=bias_variable([32])
h_conv1=tf.nn.relu(conv2d(x_reshape,w_conv1)+b_conv1)#10*20*32
h_pool1=max_pool_2x2(h_conv1)   #5*10*32

h_conv1_r=tf.nn.relu(conv2d(r_reshape,w_conv1)+b_conv1)
h_pool1_r=max_pool_2x2(h_conv1_r)
# ##conv2##
# w_conv2=weight_variable([5,5,32,64])
# b_conv2=bias_variable([64])
# h_conv2=tf.nn.relu(conv2d(x_reshape,w_conv2))#10*20*64
# h_pool2=max_pool_2x2(h_conv2)   #2.5*5*64
# 
# h_conv1_r=tf.nn.relu(conv2d(r_reshape,w_conv1))
# h_pool1_r=max_pool_2x2(h_conv1_r)

## fc1 layer ##

W_fc1 = weight_variable([5*10*32, 1024])
b_fc1 = bias_variable([1024])

h_pool1_flat=tf.reshape(h_pool1,[-1,5*10*32])
h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)

h_pool1_flat_r=tf.reshape(h_pool1_r,[-1,5*10*32])
h_fc1_r=tf.nn.relu(tf.matmul(h_pool1_flat_r, W_fc1) + b_fc1)
##fc2 layer
W_fc2 = weight_variable([1024, 200])
b_fc2 = bias_variable([200])
prediction = tf.matmul(h_fc1, W_fc2) + b_fc2


prediction_r = tf.matmul(h_fc1_r, W_fc2) + b_fc2

##train M
M=weight_variable([200,200])

rp=tf.matmul(prediction,M)

y_prediction=tf.nn.sigmoid(rp*prediction_r)

# cross_entropy=tf.reduce_mean(tf.reduce_sum(-y*tf.log(y_prediction)-(1-y)*tf.log(1-y_prediction)))
cross_entropy=-tf.reduce_mean(y*tf.log(y_prediction)+(1-y)*tf.log(1-y_prediction))
train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
# train_step = tf.train.AdagradOptimizer(0.001).minimize(cross_entropy)
##train##
sess=tf.Session()
init=tf.global_variables_initializer()
sess.run(init)
#问题向量 获取向量时使用

def get_result_q(file_path):
    with open(file_path,'r') as infile:
        content = infile.readlines()
    cleanresult=opendata.cleanText(content)
    result_p=opendata.labelizeReviews(cleanresult,'train')
    return result_p   
    print(result_p)

#答案向量 获取向量时使用
# answer_file='answer_file.txt'
 
def get_result_a(file_path):                                                #需要转化向量文件
    with open(file_path,'r') as infile:
        content = infile.readlines()
    cleanresult_a=opendata.cleanText(content)
    result_a=opendata.labelizeReviews(cleanresult_a, 'train')
    return result_a   

#获取向量的model。
model_dm = gensim.models.Doc2Vec.load('D2V.model')
model_dm_a = gensim.models.Doc2Vec.load('D2VA.model')

question_file='train_corpus_seg\simplefactquestion\questionfile.txt' 
answer_file='train_corpus_seg\simplefactquestion\questionfilee.txt'   
result_q=get_result_q(question_file)
result_a=get_result_a(answer_file)
for i in range(700):
    x_all=[np.array(model_dm.docvecs[z.tags[0]]).reshape((200)) for z in result_q] #[21,200]
    #y_all_=[[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0]]#label value
    #构造标签
    y_label = np.concatenate((np.ones(67,dtype=int), np.zeros(67,dtype=int)))
    y_=np.array(np.reshape(y_label,[134,1]))#21个正例，21个反例
    y_all=y_.tolist()#转化为list列表
   
    r_all=[np.array(model_dm_a.docvecs[z.tags[0]]).reshape((200)) for z in result_a]                                                                                #response 
    sess.run(train_step,feed_dict={x:x_all,y:y_all,r:r_all})
    if i%50==0:
        print(str(i)+':'+str(sess.run(cross_entropy,feed_dict={x:x_all,y:y_all,r:r_all})))








